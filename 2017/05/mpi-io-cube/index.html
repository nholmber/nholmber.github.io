

  
    
  


  





  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.26 with theme Tranquilpeak 0.4.3-BETA">
    <title>Parallelizing I/O operations with MPI: A case study with volumetric data</title>
    <meta name="author" content="Nico Holmberg">
    <meta name="keywords" content="MPI, computational chemistry">

    <link rel="icon" href="https://nholmber.github.io/favicon.ico">
    

    
    <meta name="description" content="Lately, parsing volumetric data from large (&gt; 300 MB) text files has been a computational bottleneck in my simulations. Because I expect to be processing hundreds of these files, I decided to parallelize the parser routine by leveraging the message passing interface (MPI). I will describe my first experience with MPI I/O in this post by going through the synthesis process of the parallelized parser routine. I will also examine the performance of the parallel parser.

">
    <meta property="og:description" content="Lately, parsing volumetric data from large (&gt; 300 MB) text files has been a computational bottleneck in my simulations. Because I expect to be processing hundreds of these files, I decided to parallelize the parser routine by leveraging the message passing interface (MPI). I will describe my first experience with MPI I/O in this post by going through the synthesis process of the parallelized parser routine. I will also examine the performance of the parallel parser.

">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Parallelizing I/O operations with MPI: A case study with volumetric data">
    <meta property="og:url" content="/2017/05/mpi-io-cube/">
    <meta property="og:site_name" content="Nico Holmberg">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Nico Holmberg">
    <meta name="twitter:description" content="Lately, parsing volumetric data from large (&gt; 300 MB) text files has been a computational bottleneck in my simulations. Because I expect to be processing hundreds of these files, I decided to parallelize the parser routine by leveraging the message passing interface (MPI). I will describe my first experience with MPI I/O in this post by going through the synthesis process of the parallelized parser routine. I will also examine the performance of the parallel parser.

">
    
    

    
    

    
      <meta property="og:image" content="https://res.cloudinary.com/nholmber/image/upload/v1533803085/img-600x600_buh1cy.png">
    

    
      <meta property="og:image" content="https://nholmber.github.io/false">
    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://nholmber.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-92528914-1', 'auto');
ga('send', 'pageview');
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://nholmber.github.io/">Nico Holmberg</a>
  </div>
  <link rel="shortcut icon" href="https://nholmber.github.io/favicon.ico?v=kPP08pKEXB" type="image/x-icon" />
  
    
      <a class="header-right-picture "
         href="https://nholmber.github.io/#about">
    
    
    
      
        <img class="header-picture" src="https://res.cloudinary.com/nholmber/image/upload/v1487079940/logo-circle-500x500_ap480i.png" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://nholmber.github.io/#about">
          <img class="sidebar-profile-picture" src="https://res.cloudinary.com/nholmber/image/upload/v1533803085/img-600x600_buh1cy.png" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Nico Holmberg</h4>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://nholmber.github.io/profile">
    
      <i class="sidebar-button-icon fa fa-lg fa-user"></i>
      
      <span class="sidebar-button-desc">Profile</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://nholmber.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://nholmber.github.io/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://nholmber.github.io/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://nholmber.github.io/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-center">
  
    <h1 class="post-title" itemprop="headline">
      Parallelizing I/O operations with MPI: A case study with volumetric data
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2017-05-04T00:00:00Z">
        
  May 4, 2017

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="https://nholmber.github.io/categories/parallel-programming">parallel programming</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>Lately, parsing volumetric data from large (&gt; 300 MB) text files has been a computational bottleneck in my simulations. Because I expect to be processing hundreds of these files, I decided to parallelize the parser routine by leveraging the <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">message passing interface</a> (MPI). I will describe my first experience with MPI I/O in this post by going through the synthesis process of the parallelized parser routine. I will also examine the performance of the parallel parser.</p>

<p></p>

<h1 id="table-of-contents">Table of Contents</h1><nav id="TableOfContents">
<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#parsing-volumetric-data-moving-from-serial-to-parallel">Parsing volumetric data: moving from serial to parallel</a></li>
<li><a href="#performance-analysis">Performance analysis</a></li>
<li><a href="#conclusions">Conclusions</a></li>
</ul>
</nav>

<h1 id="intro">Introduction</h1>

<p>The first time I had to directly interact with MPI code was when I began implementing so-called <a href="https://manual.cp2k.org/trunk/CP2K_INPUT/FORCE_EVAL/MIXED/MIXED_CDFT.html">mixed constrained DFT features into CP2K</a>, which essentially boiled down to moving and shuffling data between multiple processor groups. Admittedly, this process mainly involved using pre-existing wrappers to MPI routines to achieve what I wanted and I had to create just a few wrappers of my own. The exact details are, however, not that important in this context. I consider a clearer understanding of what is happening during MPI parallel simulations as the greatest benefit of this implementation process. Up till then, the inner workings of MPI parallel codes were very much a black box for me because I had no prior hands-on experience with MPI and the topic was only briefly covered during my chemist&rsquo;s studies. Since then I&rsquo;ve grown increasingly accustomed with MPI, although I&rsquo;ve just scratched the surface of the capabilities of MPI, but to this date I have never used MPI for file I/O.</p>

<p>In this post, I will apply MPI I/O to parallelize a routine that reads three dimensional volumetric data from a Gaussian cube file. I won&rsquo;t be reproducing the complete code in full detail to keep this post as general as possible. Instead I will describe the routine and the computational problem it strives to solve in pseudocode that borrows elements from Python and Fortran. The full code has been integrated into the <a href="https://github.com/cp2k/cp2k/blob/master/cp2k/src/pw/realspace_grid_cube.F#L353">development version of CP2K</a> for those that it interests.</p>

<p>Assume you have a function <code>f(x,y,z)</code> and that you know its values on a discrete, rectangular three dimensional grid. This grid could be very large so you&rsquo;ve decided that each processor should hold only a small subset of values of the full function. In this example, we assume that the grid can be distributed onto the processors either in one or two dimensions, thus, each processor holds either full <code>yz</code> slabs (1D processor distribution) or <code>z</code> (2D) slices of the data. The values of the function can be stored into a Gaussian cube file by iterating through the grid in row-major order and adding a line break after every 6th function value or when the last value along the <code>z</code> axis is reached (for fixed <code>{x,y}</code>). Each function value is printed in scientific notation using the format code <code>E13.5</code>. An actual Gaussian cube file would contain additional header lines that precede the volumetric data but they are irrelevant in the current context. In pseudocode, the cube file printing routine can be expressed in serial (undistributed grid) mode as</p>


  
  
  
  


<figure class="highlight python language-python">
  <figcaption>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-python"><code class="python"># Determine bounds of data
lbound = lower_bounds(f(x,y,z))
ubound = upper_bounds(f(x,y,z))

# Output format: line break every 6th value
# or when last value along z-direction
# for this value of {i, j} is printed (6E13.5)
for i in range(lbound(1), ubound(1)):
    for j in range(lbound(2), ubound(2)):
    	write(output, format) f(i, j, :)</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>

<h1 id="parsing-volumetric-data-moving-from-serial-to-parallel">Parsing volumetric data: moving from serial to parallel</h1>

<p>For the remainder of this post, assume again that the 3D grid is distributed onto an arbitrary number of processors in a fixed layout. Given a cube file of function values which is commensurate with the 3D grid, our goal is to parse the file and to distribute the data to the correct processors as efficiently as possible. The simplest way to achieve this is to read the file only on the master MPI process. Specifically, the master rank processes the file by going through it in order one <code>z</code> slice at a time and then sends the data slices to their actual owner using point-to-point communication routines (either blocking or non-blocking). Remember that by earlier assumption the <code>z</code> slices are the smallest indivisible chunks of data, <em>i.e.</em>, data along this axis is never subdivided onto multiple processors for fixed values of <code>x</code> and <code>y</code>. The process of reading and communicating the data can be sequential (repeated individual reads followed by a communication step) or interleaved. I will consider only the former possibility and call this routine &lsquo;serial&rsquo; because only the master process reads the file. This routine resembles the routine from the <a href="#intro">Introduction section</a> which was used for writing the volumetric data into a cube file</p>


  
  
  
  


<figure class="highlight python language-python">
  <figcaption>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-python"><code class="python"># Determine bounds of data
lbound = lower_bounds(f(x,y,z))
ubound = upper_bounds(f(x,y,z))

# Get MPI rank of this process, the total number of processes,
# and the rank of the master process
my_rank = MPI_get_rank()
max_rank = MPI_max_rank()
master = MPI_master_rank()

# Get the sets of grid points that each process owns
grid_point_sets = get_grid_points()

# Parse data
for i in range(lbound(1), ubound(1)):
    for j in range(lbound(2), ubound(2)):
        if my_rank = master:
            # Read function values f(i, j, :)
            read(inputfile, format) buffer

        # Determine rank of target processes who owns the data
        for ip in range(1, max_rank):
            if (i, j) in grid_point_sets(ip):
                target = ip

        # Send/Receive data
        if my_rank = master and target != my_rank:
            MPI_send(data=buffer, target=target, source=master, ...)
        else:
            # Master owns data -&gt; use explicit copy
            input_buffer = buffer
            # Type conversion
            f(i, j, :) = string_to_float(input_buffer)

        if my_rank = target and my_rank != master :
            MPI_recv(data=input_buffer, source=master, ...)
            f(i, j, :) = string_to_float(input_buffer)</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>

<p>An obvious simplification of the above routine involves removing the communication step by directly reading the appropriate sections of the cube file individually on every processor. Instead of associating multiple filehandles with the file, which would be the result if each process called an intrinsic file opening routine, the file can be opened in parallel using <code>MPI_File_open</code>. A bookkeeping scheme is required to ensure each process reads data only local to it. A simple <code>offset</code> parameter given in bytes relative to the start of the file is sufficient for this purpose. The value of <code>offset</code> is globally incremented after each <code>z</code> slice is parsed with the slice byte length (= the number of entries in the slice multiplied by their length + the number of line breaks, see above). The noncollective MPI routine <code>MPI_File_read_at</code> can then be employed to read in the data. The resulting routine dubbed &lsquo;noncollective&rsquo; can be expressed in pseudocode as</p>


  
  
  
  


<figure class="highlight python language-python">
  <figcaption>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-python"><code class="python"># Determine bounds of data
lbound = lower_bounds(f(x,y,z))
ubound = upper_bounds(f(x,y,z))

# Get MPI rank of this process
my_rank = MPI_get_rank()

# Open file with MPI
MPI_File_open(filehandle, filename, communicator_handle, ...)

# Get the set of grid points this process owns
grid_point_sets = get_grid_points()
my_grid_points = grid_point_sets(my_rank)

# Calculate byte length of data z-slices
msglen = (ubound(3)-lbound(3)&#43;1)*size_of_entry&#43;num_linebreaks

# Initialize byte offset
BOF = 0

# Parse data
for i in range(lbound(1), ubound(1)):
    for j in range(lbound(2), ubound(2)):
        if (i, j) in my_grid_points:
            # Read function values f(i, j, :)
            MPI_File_read_at(filehandle, input_buffer, ...)
            # Type conversion
            f(i, j, :) = string_to_float(input_buffer)

        # Update byte offset
        BOF &#43;= msglen</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>

<p>The final routine I will consider in this post is a variant of the &lsquo;noncollective&rsquo; routine, where the noncollective MPI file read is replaced by the collective routine <code>MPI_File_read_all</code>. Using this routine requires slightly more setup but it should perform better because collectively the routine sees the file as contiguous data. More information about these and other MPI I/O routines can be found online. These <a href="http://wgropp.cs.illinois.edu/courses/cs598-s16/lectures/lecture32.pdf"><strong>lecture notes</strong></a> by William Gropp, for example, discuss the differences between available MPI I/O routines.</p>

<p>The &lsquo;collective&rsquo; routine, which I won&rsquo;t be writing in pseudocode, involves the following steps</p>

<p>1) Calculate byte offsets (as above) to all processor local <code>z</code> slices and store them in an array whose length equals the total number of slices</p>

<p>2) Create another array of equal length to store the byte lengths of each slice (constant, in this case)</p>

<p>3) Create a suitable MPI datatype for reading in the cube file in one read call (array of strings, with each array element corresponding to a slice)</p>

<p>4) Set a file view with the created MPI datatype</p>

<p>5) Read the file</p>

<h1 id="performance-analysis">Performance analysis</h1>

<p>I&rsquo;ve compared the performance of the parallel routines against the serial reference on the following systems</p>

<ul>
<li>Local workstation: 4-core (+ 4 with HT) Intel Xeon E3-1230 V2 and 16 GB memory</li>
<li>Sisu supercomputer: 2 × 12-core Intel Xeon E5-2690v3 and 64 GB memory (per computational node), dedicated parallel distributed filesystem (Lustre)</li>
</ul>

<p>For the first benchmark, I will be using two small Gaussian cube files each consisting of 96 × 96 × 96 values. The cube files will be parsed as part of <a href="https://github.com/cp2k/cp2k/blob/master/cp2k/tests/QS/regtest-cdft-2/HeH-cdft-2.inp">a regression test</a> of the CP2K quantum chemistry code and I will report timing data only for the parser routine <code>cube_to_pw</code>. I benchmarked all three routines discussed in this post by varying the number of MPI processes. The results are tabulated below.</p>

<p><strong>Table 1.</strong> Time needed to parse a Gaussian cube as the function of the number of MPI processes with different parser routines on two different machines. The values are averages from parsing two files each consisting of 96 × 96 × 96 values.</p>

<table>
<thead>
<tr>
<th align="left"># MPI <br/> proc.</th>
<th align="center"><br/> <code>Serial</code></th>
<th align="center">Local <br/> <code>Read_at</code></th>
<th align="center"><br/> <code>Read_all</code></th>
<th align="center"><br/> <code>Serial</code></th>
<th align="center">Sisu <br/><code>Read_at</code></th>
<th align="center"><br/> <code>Read_all</code></th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">1</td>
<td align="center">0.48</td>
<td align="center">15.99</td>
<td align="center">0.63</td>
<td align="center">0.46</td>
<td align="center">0.87</td>
<td align="center">0.50</td>
</tr>

<tr>
<td align="left">2</td>
<td align="center">0.54</td>
<td align="center">8.42</td>
<td align="center">0.25</td>
<td align="center">0.47</td>
<td align="center">0.44</td>
<td align="center">0.26</td>
</tr>

<tr>
<td align="left">4</td>
<td align="center">0.68</td>
<td align="center">6.89</td>
<td align="center">0.14</td>
<td align="center">0.50</td>
<td align="center">0.24</td>
<td align="center">0.15</td>
</tr>

<tr>
<td align="left">8</td>
<td align="center">1.49</td>
<td align="center">13.89</td>
<td align="center">0.12</td>
<td align="center">0.53</td>
<td align="center">0.13</td>
<td align="center">0.08</td>
</tr>

<tr>
<td align="left">12</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">0.57</td>
<td align="center">0.10</td>
<td align="center">0.07</td>
</tr>

<tr>
<td align="left">24</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">0.68</td>
<td align="center">0.07</td>
<td align="center">0.05</td>
</tr>
</tbody>
</table>

<p>On my local workstation, which does not have a dedicated parallel distributed filesystem, the noncollective (<code>MPI_Read_at</code>) parser routine is obviously very slow compared to the serial routine. By contrast, the collective (<code>MPI_Read_all</code>) routine performs significantly better than the serial version on both machines as the number of MPI processes is increased, resulting in a factor of 10 decrease in the time needed to process the Gaussian cube file. On Sisu, the noncollective routine is also faster than the serial parser but it is outperformed by the collective routine.</p>

<p>The tests above were conducted with cube files that are notably smaller than those I will be parsing in my computational workflow. For a more realistic benchmark, I will be processing cube files with 297 × 297 × 297 entries, focusing solely on the collective parallel and serial parser routines. These tests will only be run on Sisu. The results are presented in Table 2.</p>

<p><strong>Table 2.</strong> Time needed to parse a Gaussian cube as the function of the number of MPI processes with different parser routines on Sisu. The values are averages from parsing four files each consisting of 297 × 297 × 297 values.</p>

<table>
<thead>
<tr>
<th align="left"># MPI processes</th>
<th align="center"><code>Serial</code></th>
<th align="center"><code>MPI_Read_all</code></th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">48</td>
<td align="center">33.31</td>
<td align="center">1.38</td>
</tr>

<tr>
<td align="left">96</td>
<td align="center">60.55</td>
<td align="center">0.69</td>
</tr>

<tr>
<td align="left">192</td>
<td align="center">106.35</td>
<td align="center">0.59</td>
</tr>
</tbody>
</table>

<p>With larger cube files, the difference between the serial and parallel parser routines is even more apparent. The serial parser performs ever slower as the number of MPI processes is increased. The parallel routine scales linearly from 48 to 96 MPI processes, thereafter saturating once even more processes are utilized with scaling limited by the size of the cube file. Overall, the parallel parser is a fantastic two orders of magnitude faster than the serial routine with &gt;96 MPI processes. Notice that I did not perform this benchmark with less than 48 MPI processes (or indeed just one) because the rest of the machinery within the quantum chemistry code that I am using scales well to 192 cores for this particular system.</p>

<h1 id="conclusions">Conclusions</h1>

<p>In this post, I parallelized a volumetric data parser with MPI I/O which lead to a significant performance boost for massively parallel simulations. My benchmarks clearly demonstrated the benefits of using MPI I/O for writing or reading large data sets in MPI parallel applications. The threshold for implementing I/O routines with MPI is low and requires just a moderate understanding of the message passing paradigm.</p>
              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://nholmber.github.io/tags/cp2k/">CP2K</a>

  <a class="tag tag--primary tag--small" href="https://nholmber.github.io/tags/mpi/">MPI</a>

  <a class="tag tag--primary tag--small" href="https://nholmber.github.io/tags/computational-chemistry/">computational chemistry</a>

  <a class="tag tag--primary tag--small" href="https://nholmber.github.io/tags/high-performance-computing/">high-performance computing</a>

                  </div>
                
              
            
            
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://nholmber.github.io/2017/12/mpi-io-cube-pt2/" data-tooltip="Parallelizing I/O operations with MPI, Part 2: Performance tuning and Cray Burst Buffer">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://nholmber.github.io/2017/04/cp2k-build-cray-xc40/" data-tooltip="Building CP2K on a Cray XC40">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://nholmber.github.io/2017/05/mpi-io-cube/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://nholmber.github.io/2017/05/mpi-io-cube/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=https://nholmber.github.io/2017/05/mpi-io-cube/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#table-of-contents">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2018 Nico Holmberg. Powered by <a href="https://gohugo.io">Hugo</a> and the <a href="https://themes.gohugo.io/hugo-tranquilpeak-theme">Tranquilpeak</a> theme. All Rights Reserved
  </span>

  <script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
		MathJax.Hub.Config({
		tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']],
		displayMath: [['$$','$$']],
		processEscapes: true,
		processEnvironments: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
		TeX: { equationNumbers: { autoNumber: "AMS" },
		     extensions: ["AMSmath.js", "AMSsymbols.js"] }
		}
		});
		MathJax.Hub.Queue(function() {
		
		
		
		var all = MathJax.Hub.getAllJax(), i;
		for(i = 0; i < all.length; i += 1) {
		    all[i].SourceElement().parentNode.className += ' has-jax';
		}
		});
		MathJax.Hub.Config({
		
		TeX: { equationNumbers: { autoNumber: "AMS" } }
		});
	</script>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://nholmber.github.io/2017/12/mpi-io-cube-pt2/" data-tooltip="Parallelizing I/O operations with MPI, Part 2: Performance tuning and Cray Burst Buffer">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://nholmber.github.io/2017/04/cp2k-build-cray-xc40/" data-tooltip="Building CP2K on a Cray XC40">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://nholmber.github.io/2017/05/mpi-io-cube/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://nholmber.github.io/2017/05/mpi-io-cube/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=https://nholmber.github.io/2017/05/mpi-io-cube/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#table-of-contents">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fnholmber.github.io%2F2017%2F05%2Fmpi-io-cube%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fnholmber.github.io%2F2017%2F05%2Fmpi-io-cube%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=https%3A%2F%2Fnholmber.github.io%2F2017%2F05%2Fmpi-io-cube%2F">
          <i class="fa fa-google-plus"></i><span>Share on Google&#43;</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://res.cloudinary.com/nholmber/image/upload/v1533803085/img-600x600_buh1cy.png" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Nico Holmberg</h4>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Doctoral Student, Computational Chemistry
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Espoo, Finland
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2018/08/gmaps-statistics/">
                <h3 class="media-heading">Visualizing Geographic Statistical Data with Google Maps</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Aug 8, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>This tutorial will teach you how to create a custom Google Maps based map for visualizing geographic statistical data. Such maps can be a useful tool when developing machine learning models. As a specific example case, we will create a map for visualizing the population density and median household income of postal code areas in Finland.</p>

<p></p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2018/05/mpi-diagonalization/">
                <h3 class="media-heading">Matrix diagonalization in parallel computing: Benchmarking ELPA against ScaLAPACK</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  May 5, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>Matrix diagonalization is a fundamental linear algebra operation with a wide range of applications in scientific and other fields of computing. At the same time, it is also one of the most expensive operations with a formal <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">computational complexity</a> of $\mathcal{O}(N^3)$, which can become a significant performance bottleneck as the size of the system grows. In this post, I will introduce the canonical algorithm for diagonalizing matrices in parallel computing to set the scene for today&rsquo;s main topic: improving diagonalization performance. With the help of benchmark calculations, I will then demonstrate how a clever mathematical library choice can easily reduce the time needed to diagonalize a matrix by at least 50 %.</p>

<p></p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2018/01/cp2k-uk-2018-recap/">
                <h3 class="media-heading">A recap of the CP2K-UK 2018 meeting</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Jan 1, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>The 2018 edition of the <a href="https://www.cp2k.org/events:2018_user_meeting:index">CP2K UK user meeting</a> was held two Fridays ago at the University of Lincoln, UK. I will be recaping my experiences of the event in this post.</p>

<p></p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2017/12/cp2k-uk-2018/">
                <h3 class="media-heading">Constrained DFT tutorial at the CP2K-UK 2018 meeting</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2017
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>I will be giving an introductory how-to talk on constrained DFT at the <a href="https://tinyurl.com/CP2K-2018">2018 CP2K UK User meeting</a> at the University of Lincoln, UK, on Friday January 12 2018.</p>

<p></p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2017/12/mpi-io-cube-pt2/">
                <h3 class="media-heading">Parallelizing I/O operations with MPI, Part 2: Performance tuning and Cray Burst Buffer</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2017
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>In my previous <a href="https://nholmber.github.io/2017/05/mpi-io-cube/">post</a>, I discussed the benefits of using the message passing interface (MPI) to parse large data files in parallel over multiple processors. In today&rsquo;s post, I will demonstrate how MPI I/O operations can be further accelerated by introducing the concept of hints. The second topic I will discuss is the emergence of solid-state drives in high-performance computing systems to resolve I/O bottlenecks. These topics will be illustrated by benchmark calculations using a parallel writer routine that I will implement to the <a href="https://nholmber.github.io/2017/05/mpi-io-cube/">task described previously</a>.</p>

<p></p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2017/05/mpi-io-cube/">
                <h3 class="media-heading">Parallelizing I/O operations with MPI: A case study with volumetric data</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  May 5, 2017
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>Lately, parsing volumetric data from large (&gt; 300 MB) text files has been a computational bottleneck in my simulations. Because I expect to be processing hundreds of these files, I decided to parallelize the parser routine by leveraging the <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">message passing interface</a> (MPI). I will describe my first experience with MPI I/O in this post by going through the synthesis process of the parallelized parser routine. I will also examine the performance of the parallel parser.</p>

<p></p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2017/04/cp2k-build-cray-xc40/">
                <h3 class="media-heading">Building CP2K on a Cray XC40</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Apr 4, 2017
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>Compiling a <a href="https://www.cp2k.org/">CP2K</a> binary &ndash; the massively parallel, open-source quantum chemistry and solid state physics program written in Fortran 2003 &ndash; can be a daunting task if you&rsquo;ve never built a large project using <a href="https://en.wikipedia.org/wiki/Make_(software)">maketools</a>. This post aims to demystify the build process by showcasing a full CP2K build on a Cray XC40 supercomputer (codename <a href="https://research.csc.fi/sisu-supercomputer"><em>Sisu</em></a>) including dependencies.
</p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://nholmber.github.io/2017/02/welcome/">
                <h3 class="media-heading">Welcome to my website!</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Feb 2, 2017
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather"><p>Greetings and welcome! You have stumbled upon the personal website and blog of Nico Holmberg, a 26 year old Finn pursuing a doctoral degree in Computational Quantum Chemistry. Feel free to check out my <a href="../../../profile">profile</a> for more information about me, my professional projects and other interests!
</p></div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         8 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://res.cloudinary.com/nholmber/image/upload/v1487095700/sidebar_nefvej.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://nholmber.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>



<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = 'https:\/\/nholmber.github.io\/2017\/05\/mpi-io-cube\/';
          
            this.page.identifier = '\/2017\/05\/mpi-io-cube\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'nholmber';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  



    
  </body>
</html>

